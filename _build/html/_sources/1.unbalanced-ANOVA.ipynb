{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# Unbalanced ANOVA Models\n",
    "\n",
    "```{figure} images/unbalanced-text.webp\n",
    "---\n",
    "scale: 80%\n",
    "align: right\n",
    "---\n",
    "```\n",
    "\n",
    "... Indeed, whole textbooks were written about unbalanced data (as can be seen on the *right*). So this is a topic that deserves some attention, even if it is largely *ignored* by modern teaching in Psychology. There is something of an assumption that the issues of balance have been *solved* and thus do not need considering anymore. However, this is not really true. The \"solution\" implemented by SAS and SPSS is the Type III sums-of-squares, which researchers continue to use because it is the default[^default-foot]. However, as discussed briefly last week, this approach is highly flawed.\n",
    "\n",
    "In this part of the lesson, we will dig deeper into the Type I/II/III debate so that you understand what each type of sums-of-squares means, when they are most appropriate to use and what the various arguments are for/against them. In general, we will be recommending Type II for 95% of all use-cases. However, it is important not to just take our word for it. Instead, it is important that you *understand* the difference and can make your own informed judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431a6c5",
   "metadata": {},
   "source": [
    "## The Problem of Imbalance\n",
    "... Perhaps the most important thing to recognise here is that imbalance is only a problem when we insist on trying to interpret effects that *do not make sense* in the context of the model. For instance, trying to interpret a main effect in the presence of an interaction. If an interaction effect is *large* then the main effects make no sense however, when the interaction effect is *small*, it adds little to the predictive accuracy of the model and should not be there. \n",
    "\n",
    "The key point here is that all this hassle goes away if we just engage with the idea of *model building* and only interpret tests once we have a suitable model in place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626aa56",
   "metadata": {},
   "source": [
    "## The Principle of Marginality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a741b7",
   "metadata": {},
   "source": [
    "## Type I Sums-of-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934b6e9",
   "metadata": {},
   "source": [
    "## Type II Sums-of-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51260ac5",
   "metadata": {},
   "source": [
    "## Type III Sums-of-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b548d",
   "metadata": {},
   "source": [
    "## Resolving the Sums-of-squares Circus\n",
    "... The truth is that the main reason all this hassle exists is because the neat partition of the ANOVA effects disappears when the data are imbalanced. In order to resolve this, we have to choose a method of partitioning the sums-of-squares. The definitions given above come directly from SAS, who's aim was not some principled statistical derivation that makes sense, rather it was to give their users what they wanted: identical ANOVA output irrespective of balance. Because the traditional ANOVA was not seen as an exercise in model building, it was not typical to remove terms that appeared redundant. In order to maintain this completeness, SAS wanted ANOVA tables that contained *all* terms, rather than certain terms disappearing under imbalance. As such, different methods for decomposing these effects were developed and a choice was provided. \n",
    "\n",
    "From a modern perspective, all this hassle is unnecessary if we engage with the process of *model building*. This is something we will discuss in much greater detail in the machine learning module next semester. However, the idea is very simple. If a term adds little predictive utility, remove it and create the simplest model you can. From this perspective, if the highest-order interaction is *small* it would be removed and then the lower-order terms become interpretable again. No need for Type II tests to make them intepretable *despite* the presence of the interaction term. However, if an interaction is *large*, it stays in the model and we only interpret the highest-order term for each factor. Under this scheme, the whole Type I/II/III debate disappears. \n",
    "\n",
    "As an example, say we have the model \n",
    "\n",
    "$$\n",
    "Y = A + B + C + AB + AC + BC + ABC.\n",
    "$$\n",
    "\n",
    "If the 3-way interaction is uninteresting, we can drop it to form\n",
    "\n",
    "$$\n",
    "Y = A + B + C + AB + AC + BC.\n",
    "$$\n",
    "\n",
    "Now, say that $AC$ and $BC$ are also uninteresting, we can settle on\n",
    "\n",
    "$$\n",
    "Y = A + B + C + AB.\n",
    "$$\n",
    "\n",
    "We would now interpret the 2-way interaction $AB$ and the main effect $C$. Because we have respected marginality here when building these models, all these terms have interpretable effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8f2aaf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova Table (Type II tests)\n",
      "\n",
      "Response: pain_score\n",
      "                       Sum Sq Df F value    Pr(>F)    \n",
      "gender                 313.36  1 16.1957 0.0001625 ***\n",
      "risk                  1793.56  1 92.6988   8.8e-14 ***\n",
      "treatment              283.17  2  7.3177 0.0014328 ** \n",
      "gender:risk              2.73  1  0.1411 0.7084867    \n",
      "gender:treatment       129.18  2  3.3384 0.0422001 *  \n",
      "risk:treatment          27.60  2  0.7131 0.4942214    \n",
      "gender:risk:treatment  286.60  2  7.4063 0.0013345 ** \n",
      "Residuals             1160.89 60                      \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "Analysis of Variance Table\n",
      "\n",
      "Response: pain_score\n",
      "               Df  Sum Sq Mean Sq F value    Pr(>F)    \n",
      "gender          1  313.36  313.36 12.8962 0.0006333 ***\n",
      "risk            1 1793.56 1793.56 73.8135 2.607e-12 ***\n",
      "treatment       2  283.17  141.58  5.8269 0.0047027 ** \n",
      "risk:treatment  2   27.60   13.80  0.5678 0.5695375    \n",
      "Residuals      65 1579.40   24.30                      \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "Anova Table (Type II tests)\n",
      "\n",
      "Response: pain_score\n",
      "                Sum Sq Df F value    Pr(>F)    \n",
      "gender          313.36  1 12.8962 0.0006333 ***\n",
      "risk           1793.56  1 73.8135 2.607e-12 ***\n",
      "treatment       283.17  2  5.8269 0.0047027 ** \n",
      "risk:treatment   27.60  2  0.5678 0.5695375    \n",
      "Residuals      1579.40 65                      \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "Anova Table (Type III tests)\n",
      "\n",
      "Response: pain_score\n",
      "               Sum Sq Df    F value    Pr(>F)    \n",
      "(Intercept)    434108  1 17865.6292 < 2.2e-16 ***\n",
      "gender            313  1    12.8962 0.0006333 ***\n",
      "risk             1794  1    73.8135 2.607e-12 ***\n",
      "treatment         283  2     5.8269 0.0047027 ** \n",
      "risk:treatment     28  2     0.5678 0.5695375    \n",
      "Residuals        1579 65                         \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "library('datarium')\n",
    "library('car')\n",
    "data(headache)\n",
    "mod <- lm(pain_score ~ gender*risk*treatment, data=headache)\n",
    "print(Anova(mod))\n",
    "\n",
    "mod <- lm(pain_score ~ gender + risk + treatment + risk:treatment, data=headache)\n",
    "\n",
    "mod.sum <- lm(pain_score ~ gender + risk + treatment + risk:treatment, data=headache, contrasts=list(gender=contr.sum,risk=contr.sum,treatment=contr.sum))\n",
    "\n",
    "print(anova(mod))\n",
    "print(Anova(mod))\n",
    "print(Anova(mod.sum, type=\"III\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63c7bb",
   "metadata": {},
   "source": [
    "`````{topic} What do you now know?\n",
    "In this section, we have explored ... After reading this section, you should have a good sense of:\n",
    "\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78f7a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "[^default-foot]: Always be wary of defaults. If there is one way of getting an entire scientific field to adhere to a particular way of doing something without the need for any critical evaluation, simply make it the default in software. Defaults do not automatically hold some higher-level of credibility simply because they were the value that the developer picked. Many times these are well-considered, but this is not a *guarantee*. We can easily be led astray by default choices because we do not have to justify using them. This does not have an official name, but we could perhaps call it *the default authority effect*. It is effectively a reversal of the burden of proof: deviating from defaults requires defence, whereas using defaults is treated as neutral. Yet this presupposes that the defaults are normatively sound, which is rarely demonstrated or even documented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba3326",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
