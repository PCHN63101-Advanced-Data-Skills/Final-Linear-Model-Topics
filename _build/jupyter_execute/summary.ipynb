{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a964b-a2bc-4d77-b1e8-6928da6b2b73",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This lesson brings to a close the PCHN63101 unit. We have now completed our journey from first principles of a statistical model through linear regression and ANOVA models, culminating in discussions of unbalanced ANOVA and follow-up tests within this lesson. We have covered a lot of theoretical and practical ground across the last few weeks. Although your head may still be spinning from all this, it is worthwhile taking a step back and thinking about what we have achieved. Taking everything together, we now have a singular and generic framework for analysing *any* dataset where the outcome can be treated as a continuous random variable. This is no small feat. We know exactly how to include continuous predictor variable and categorical predictor variables. We know how to deal with categorical predictor variables with multiple levels and with interactions. We have seen how to generate visualisations of the assumptions as well as alternative methods when those assumptions are violated. Indeed, no matter the data, our steps are as follows:\n",
    "\n",
    "1. Fit a model using `lm()`.\n",
    "2. Assess the assumptions using `plot(mod)`.\n",
    "3. Make adjustments and corrections if needed until we are satisfied.\n",
    "4. Call `summary(mod)` and interpret the output.\n",
    "5. Call `Anova(mod)` and interpret the output.\n",
    "6. Use `emmeans()` to generate follow-up tests.\n",
    "\n",
    "Not all these steps are always relevant. For instance, in a regression model both `Anova()` and `emmeans()` will be redundant, whereas for an ANOVA model it may not be useful to interpret the results from `summary()`. However, each step remains an option that your should now understand in the wider context of linear models. This means we do not have to think in terms of flow-charts, different analyses or tests, or any of the other piecemeal approaches to statistics. All we think about is *the model*, nothing more. We have our model and we want to ask that model questions using its estimates. Steps 1-3 correspond to making sure we can trust the model estimates. Steps 4-6 correspond to asking questions about those estimates. The only differences between steps 4, 5 and 6 is how those questions are asked. This is the real *big idea* behind all statistics. The parameters are everything, we just have choices around how we use those parameters to reach conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cee43",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}